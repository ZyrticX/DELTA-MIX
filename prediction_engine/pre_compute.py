"""
Pre-Computation Engine - ×—×™×©×•×‘ ×›×œ ×”×§×•×¨×œ×¦×™×•×ª ×”×”×™×¡×˜×•×¨×™×•×ª + ×ª×•×¦××•×ª ×¢×ª×™×“×™×•×ª
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pickle
import logging
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import time

# ×”×•×¡×¤×ª × ×ª×™×‘ ×œ××•×“×•×œ×™×
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_fetcher import DataFetcher
from correlation_engine import CorrelationEngine
from .config import COMPUTATION_PARAMS, MULTIPROCESSING_CONFIG, PATHS
from .db_client import SupabaseClient
from .utils import (
    classify_movement,
    calculate_correlation_for_date,
    calculate_future_return,
    create_pattern_signature
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PreComputeEngine:
    """
    ×× ×•×¢ Pre-Computation ×œ×—×™×©×•×‘ ×›×œ ×”×§×•×¨×œ×¦×™×•×ª ×”×”×™×¡×˜×•×¨×™×•×ª
    """
    
    def __init__(self):
        """××ª×—×•×œ"""
        self.data_fetcher = DataFetcher(cache_dir=PATHS['data_cache'])
        self.db_client = SupabaseClient()
        self.params = COMPUTATION_PARAMS
        
    def load_stock_data(self, symbols: List[str], start_date: str = "2012-01-01") -> pd.DataFrame:
        """
        ×˜×¢×™× ×ª × ×ª×•× ×™ ×× ×™×•×ª ×-cache
        
        Args:
            symbols: ×¨×©×™××ª ×¡×™××•×œ×™×
            start_date: ×ª××¨×™×š ×”×ª×—×œ×”
            
        Returns:
            DataFrame ×¢× MultiIndex (symbol, field)
        """
        logger.info(f"ğŸ“‚ ×˜×•×¢×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ {len(symbols)} ×× ×™×•×ª...")
        
        all_data = {}
        failed = []
        
        for symbol in tqdm(symbols, desc="×˜×¢×™× ×ª × ×ª×•× ×™×"):
            cache_file = os.path.join(PATHS['data_cache'], f"{symbol}_{start_date}_None.pkl")
            
            if os.path.exists(cache_file):
                try:
                    with open(cache_file, 'rb') as f:
                        df = pickle.load(f)
                    
                    if df is not None and not df.empty:
                        # ×©××™×¨×ª ×¨×§ ×©×“×•×ª ×¨×œ×•×•× ×˜×™×™×
                        for field in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']:
                            if field in df.columns:
                                all_data[(symbol, field)] = df[field]
                except Exception as e:
                    logger.warning(f"âš ï¸ ×©×’×™××” ×‘×˜×¢×™× ×ª {symbol}: {e}")
                    failed.append(symbol)
            else:
                failed.append(symbol)
        
        if failed:
            logger.warning(f"âš ï¸ {len(failed)} ×× ×™×•×ª ×œ× × ×˜×¢× ×•: {failed[:10]}...")
        
        if not all_data:
            raise ValueError("×œ× × ××¦××• × ×ª×•× ×™×!")
        
        # ×™×¦×™×¨×ª DataFrame
        stock_data = pd.DataFrame(all_data)
        stock_data.index = pd.to_datetime(stock_data.index)
        stock_data = stock_data.sort_index()
        
        logger.info(f"âœ… × ×˜×¢× ×• × ×ª×•× ×™× ×¢×‘×•×¨ {len(symbols) - len(failed)} ×× ×™×•×ª")
        logger.info(f"ğŸ“… ×˜×•×•×— ×ª××¨×™×›×™×: {stock_data.index[0]} ×¢×“ {stock_data.index[-1]}")
        
        return stock_data
    
    def compute_snapshots_for_stock(self, 
                                    stock_data: pd.DataFrame,
                                    stock: str,
                                    dates: List[datetime],
                                    all_symbols: List[str],
                                    lookback_days: int,
                                    forward_days: int,
                                    correlation_threshold: float) -> List[Dict[str, Any]]:
        """
        ×—×™×©×•×‘ snapshots ×œ×× ×™×” ××—×ª
        
        Args:
            stock_data: DataFrame ×¢× ×›×œ ×”× ×ª×•× ×™×
            stock: ×¡×™××•×œ ×”×× ×™×”
            dates: ×¨×©×™××ª ×ª××¨×™×›×™× ×œ×—×™×©×•×‘
            all_symbols: ×¨×©×™××ª ×›×œ ×”×¡×™××•×œ×™×
            lookback_days: ×™××™× ××—×•×¨×”
            forward_days: ×™××™× ×§×“×™××”
            correlation_threshold: ×¡×£ ×§×•×¨×œ×¦×™×”
            
        Returns:
            ×¨×©×™××ª snapshots
        """
        snapshots = []
        
        for date in dates:
            try:
                # ×—×™×©×•×‘ ×§×•×¨×œ×¦×™×•×ª ×¢× ×›×œ ×”×× ×™×•×ª ×”××—×¨×•×ª
                matched_stocks = []
                
                for other_stock in all_symbols:
                    if other_stock == stock:
                        continue
                    
                    # ×§×•×¨×œ×¦×™×™×ª ××—×™×¨
                    corr_price = calculate_correlation_for_date(
                        stock_data, stock, other_stock, date,
                        lookback_days, 'Adj Close'
                    )
                    
                    # ×§×•×¨×œ×¦×™×™×ª × ×¤×—
                    corr_volume = calculate_correlation_for_date(
                        stock_data, stock, other_stock, date,
                        lookback_days, 'Volume'
                    )
                    
                    # ×‘×“×™×§×” ×× ×¢×•×‘×¨ ××ª ×”×¡×£
                    if corr_price and corr_price >= correlation_threshold:
                        matched_stocks.append({
                            'symbol': other_stock,
                            'corr_price': corr_price,
                            'corr_volume': corr_volume or 0.0
                        })
                    elif corr_volume and corr_volume >= correlation_threshold:
                        matched_stocks.append({
                            'symbol': other_stock,
                            'corr_price': corr_price or 0.0,
                            'corr_volume': corr_volume
                        })
                
                # ×—×™×©×•×‘ ×ª× ×•×¢×” ×¢×ª×™×“×™×ª
                future_return = calculate_future_return(
                    stock_data, stock, date, forward_days, 'Adj Close'
                )
                
                movement_type = None
                if future_return is not None:
                    movement_type = classify_movement(
                        future_return,
                        self.params['movement_thresholds']
                    )
                
                # ×™×¦×™×¨×ª snapshot
                snapshot = {
                    'snapshot_date': date.strftime('%Y-%m-%d'),
                    'stock_symbol': stock,
                    'matched_stocks': matched_stocks,
                    'num_matches': len(matched_stocks),
                    'future_return_pct': future_return,
                    'movement_type': movement_type,
                    'lookback_days': lookback_days,
                    'forward_days': forward_days,
                    'correlation_threshold': correlation_threshold
                }
                
                snapshots.append(snapshot)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ×©×’×™××” ×‘×—×™×©×•×‘ snapshot ×¢×‘×•×¨ {stock} ×‘-{date}: {e}")
                continue
        
        return snapshots
    
    def compute_all_snapshots(self,
                            stock_data: pd.DataFrame,
                            start_date: Optional[str] = None,
                            end_date: Optional[str] = None,
                            lookback_days: Optional[int] = None,
                            forward_days: Optional[int] = None,
                            correlation_threshold: Optional[float] = None) -> List[Dict[str, Any]]:
        """
        ×—×™×©×•×‘ ×›×œ ×”-snapshots
        
        Args:
            stock_data: DataFrame ×¢× ×›×œ ×”× ×ª×•× ×™×
            start_date: ×ª××¨×™×š ×”×ª×—×œ×” (×‘×¨×™×¨×ª ××—×“×œ: ×™×•× 16)
            end_date: ×ª××¨×™×š ×¡×™×•× (×‘×¨×™×¨×ª ××—×“×œ: ×”×™×•×)
            lookback_days: ×™××™× ××—×•×¨×”
            forward_days: ×™××™× ×§×“×™××”
            correlation_threshold: ×¡×£ ×§×•×¨×œ×¦×™×”
            
        Returns:
            ×¨×©×™××ª ×›×œ ×”-snapshots
        """
        # ×¤×¨××˜×¨×™×
        lookback_days = lookback_days or self.params['lookback_days']
        forward_days = forward_days or self.params['forward_days']
        correlation_threshold = correlation_threshold or self.params['correlation_threshold']
        
        # ×ª××¨×™×›×™×
        all_dates = stock_data.index.tolist()
        
        if start_date:
            start_date = pd.to_datetime(start_date)
        else:
            # ×™×•× 16 (×¦×¨×™×š lookback_days × ×ª×•× ×™×)
            start_date = all_dates[lookback_days - 1] if len(all_dates) > lookback_days else all_dates[0]
        
        if end_date:
            end_date = pd.to_datetime(end_date)
        else:
            end_date = all_dates[-1]
        
        # ×¡×™× ×•×Ÿ ×ª××¨×™×›×™×
        dates = [d for d in all_dates if start_date <= d <= end_date]
        
        # ×¦×¨×™×š ×œ×¤×—×•×ª forward_days ××—×¨×™ ×”×ª××¨×™×š ×”××—×¨×•×Ÿ
        if len(all_dates) > 0:
            last_available_date = all_dates[-1]
            max_date = last_available_date - timedelta(days=forward_days)
            dates = [d for d in dates if d <= max_date]
        
        logger.info(f"ğŸ“… ××—×©×‘ snapshots ×¢×‘×•×¨ {len(dates)} ×ª××¨×™×›×™×")
        logger.info(f"   ×-{dates[0]} ×¢×“ {dates[-1]}")
        
        # ×× ×™×•×ª
        symbols = stock_data.columns.get_level_values(0).unique().tolist()
        logger.info(f"ğŸ“Š ××—×©×‘ ×¢×‘×•×¨ {len(symbols)} ×× ×™×•×ª")
        
        # ×—×™×©×•×‘
        all_snapshots = []
        
        # Multiprocessing
        max_workers = MULTIPROCESSING_CONFIG['max_workers']
        chunk_size = MULTIPROCESSING_CONFIG['chunk_size']
        
        # ×—×œ×•×§×” ×œ-chunks
        stock_chunks = [symbols[i:i + chunk_size] for i in range(0, len(symbols), chunk_size)]
        
        logger.info(f"âš™ï¸ ××©×ª××© ×‘-{max_workers} workers, {len(stock_chunks)} chunks")
        
        for chunk_idx, stock_chunk in enumerate(stock_chunks):
            logger.info(f"ğŸ“¦ ××¢×‘×“ chunk {chunk_idx + 1}/{len(stock_chunks)} ({len(stock_chunk)} ×× ×™×•×ª)")
            
            chunk_snapshots = []
            
            for stock in tqdm(stock_chunk, desc=f"Chunk {chunk_idx + 1}"):
                snapshots = self.compute_snapshots_for_stock(
                    stock_data, stock, dates, symbols,
                    lookback_days, forward_days, correlation_threshold
                )
                chunk_snapshots.extend(snapshots)
            
            # ×©××™×¨×” ×œ-DB
            if chunk_snapshots:
                logger.info(f"ğŸ’¾ ×©×•××¨ {len(chunk_snapshots)} snapshots ×œ-DB...")
                self.db_client.insert_correlation_snapshots(chunk_snapshots)
                all_snapshots.extend(chunk_snapshots)
        
        logger.info(f"âœ… ×”×•×©×œ×! × ×•×¦×¨×• {len(all_snapshots)} snapshots")
        
        return all_snapshots
    
    def run(self, 
           symbols: Optional[List[str]] = None,
           start_date: Optional[str] = None,
           end_date: Optional[str] = None,
           test_mode: bool = False):
        """
        ×”×¨×¦×ª Pre-Computation ××œ×
        
        Args:
            symbols: ×¨×©×™××ª ×× ×™×•×ª (×× None, ×˜×•×¢×Ÿ ×-DB)
            start_date: ×ª××¨×™×š ×”×ª×—×œ×”
            end_date: ×ª××¨×™×š ×¡×™×•×
            test_mode: ×× True, ×¨×¥ ×¨×§ ×¢×œ 10 ×× ×™×•×ª ×œ×‘×“×™×§×”
        """
        logger.info("ğŸš€ ××ª×—×™×œ Pre-Computation Engine...")
        
        # ×§×‘×œ×ª ×¨×©×™××ª ×× ×™×•×ª
        if symbols is None:
            stocks_from_db = self.db_client.get_stock_list(active_only=True)
            if stocks_from_db:
                symbols = [s['symbol'] for s in stocks_from_db]
            else:
                # Fallback ×œ-DataFetcher
                logger.warning("âš ï¸ ×œ× × ××¦××• ×× ×™×•×ª ×‘-DB, ××©×ª××© ×‘-DataFetcher...")
                symbols = self.data_fetcher.get_sp500_symbols()
        
        if test_mode:
            symbols = symbols[:10]
            logger.info(f"ğŸ§ª ××¦×‘ ×‘×“×™×§×”: ××¢×‘×“ ×¨×§ {len(symbols)} ×× ×™×•×ª")
        
        # ×˜×¢×™× ×ª × ×ª×•× ×™×
        stock_data = self.load_stock_data(symbols, start_date or "2012-01-01")
        
        # ×—×™×©×•×‘ snapshots
        snapshots = self.compute_all_snapshots(
            stock_data,
            start_date=start_date,
            end_date=end_date
        )
        
        logger.info(f"âœ… Pre-Computation ×”×•×©×œ× ×‘×”×¦×œ×—×”!")
        logger.info(f"   ğŸ“Š {len(snapshots)} snapshots × ×©××¨×• ×‘-DB")
        
        return snapshots


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DeltaMix 2.0 Pre-Computation Engine')
    parser.add_argument('--test', action='store_true', help='××¦×‘ ×‘×“×™×§×” (10 ×× ×™×•×ª)')
    parser.add_argument('--start-date', type=str, help='×ª××¨×™×š ×”×ª×—×œ×” (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='×ª××¨×™×š ×¡×™×•× (YYYY-MM-DD)')
    parser.add_argument('--symbols', nargs='+', help='×¨×©×™××ª ×× ×™×•×ª ×¡×¤×¦×™×¤×™×ª')
    
    args = parser.parse_args()
    
    engine = PreComputeEngine()
    engine.run(
        symbols=args.symbols,
        start_date=args.start_date,
        end_date=args.end_date,
        test_mode=args.test
    )


if __name__ == '__main__':
    main()

